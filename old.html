

<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
<html>
<font face="verdana, Sans-serif">
<head>
   <title>Conor Houghton</title>
   <link rel="icon"  type="image/x-icon" href="https://conorhoughton.github.io/small_star.ico?">
</head>


<body  bgcolor=white link="slategrey" vlink="darkslategrey" alink="darkslategrey">

<table>
  <tbody>
    <tr>
      <td bgcolor="slategrey" width="30">&nbsp;</td>
      
      <td bgcolor="darkslategrey" width="1">&nbsp;</td>
      <td bgcolor=white width="10">&nbsp;</td>

     <td width="15"> 
<td>
  <h1 style="color: darkslategrey">Conor Houghton</h1>
  <p><br></p>
<a href="papers"><b>Papers</b></a><br>
  <p><b>Old news</b></p>
  <p><b>(2024-09-06)</b> Poster from CCS24</p>
<p><a href="https://doi.org/10.5281/zenodo.13709968">doi.org/10.5281/zenodo.13709968</a></p>
<p>about my attempt to find the simplest possible model of language evolution, one that includes only our wish to communicate, our propensity towards playful innovation in language and our inclination to communicate only with those whose language resembles our own. The result is a sort of Ising model.</p>
<p>I amn't sure how useful this model is, it contains nothing about the mechanics of language and so perhaps it is too abstract to tell us anything useful. However one thing I did find while working on the simulations and wondering how to compare it to real data is that the languages of the world, by population, satisfy a LogNormal.</p>
  <p><b>(2024-09-05)</b> I was at the super COMPILA2024 workshop at CCS2024 this week; I really enjoyed the workshop, it was my first time meeting other people keen to see what modelling can tell us about language change. My talk was about new Iterated Learning Model I'm proposing with Seth Bullock and Jack Bunyan. I made a recording of the talk:</p>
  <p><a href="https://www.youtube.com/watch?v=kRRqZ-l9K9A">youtube</a> with slides at <a href="https://doi.org/10.5281/zenodo.13692349">doi.org/10.5281/zenodo.13692349</a></p>
  In the talk, more than the paper, I'm trying to sell the idea that the model suggests something exciting: that the key to language evolution and to the use of language specifically by humans, is the use humans make of our utterances as a component of our thought. The idea is</p>
<ul>
  <li>simple animals: stimulus &#x2192; action</li>
  <li>more complex animals: stimulus &#x2192; thought &#x2192; action</li>
  <li>social animals: stimulus &#x2192; thought &#x2192; action or utterance</li>
  <li>our ancestors: stimulus &#x2192; (thought &#x2194; internal utterances) &#x2192; action or utterance</li>
  <li>humans: stimulus  &#x2192; (thought &#x2194; internal language) &#x2192; action or language</li>
</ul>
<p>The paper is here: <a href="https://arxiv.org/abs/2405.20818">arxiv: 2405.20818</a></p>
<p><b>(2024-08-20)</b> My student Davide Turco has made a great poster for our Conference on Cognitive Computational Neuroscience paper "Investigating the timescales of language processing with EEG and language models":
<a href="https://zenodo.org/records/13134148">zenodo.org</a></p>
<p>The paper is<br>
  <i>Investigating the timescales of language processing with EEG and language models.</i><br>
  Davide Turco and Conor Houghton<br>
Conference on Cognitive Computational Neuroscience (CCN 2024)<br>  
  <a href="https://arxiv.org/abs/2406.19884">arxiv: 2406.19884</a></p>
  <p><b>(2024-08-15)</b>: A new paper under review describing a hierarchical Bayesian workflow for analysing cell count data:<br>
  <i>Hierarchical Bayesian modeling of multi-region brain cell count data.</i><br>
  Sydney Dimmock, Benjamin M.S. Exley,  Gerald Moore, Lucy Menage,  Alessio Delogu,  Simon R Schultz,  E Clea Warburton,  Conor J Houghton and  Cian O'Donnell<br>
  <a href="https://www.biorxiv.org/content/10.1101/2024.07.20.603979v1">bioRxiv</a>, bioRxiv doi: <a href=" https://doi.org/10.1101/2024.07.20.603979">10.1101/2024.07.20.603979</a></p>
  <p>These days experimentalists can mark neurons, slice up the brain and then count the marked cells. To use the two examples in our paper, this might mean marking all the cells that are active during some behaviour or all the cells with a particular developmental lineage. For each animal the number of marked cells is counted for lots of different brain region, in some experiments as many as a hundred.</p>
<p>These data are super cool, they give information across the whole brain. They are, however, very time-consuming and expensive to collect and often there are only ten animals for each experimental condition. Now, the whole cool thing about the data is the high dimension, there are cell counts for each brain region, but this combination of a high dimension and a small sample means the data are under-sampled.</p>
<p>Clearly Bayesian analysis can help with analysis, but for a Bayesian analysis you need to decide on a model and a set of priors. New samplers means Bayesian approaches can be used for data like this, but setting up the analysis can be intimating. Bayesian methods predate t-tests and the like, but in the last century they were not as well used as the classic mixture of hypothesis tests. As such there is not a lot of lore and tradition about what choices to make when it comes to doing a Bayesian analysis.</p>
<p>In our paper we try to help by suggesting a 'standard' Bayesian workflow for cell count data. We test our workflow on two datasets and in both cases it works really well. It produces clearer results than a more classical approach. Bayesian models are less familiar but they are actually very transparent and clear.</p>
  <p><b><a href="./index.html">Back to main page</a><b></b>
<p><br></p>
</td>
  </tbody>
</table>
</body>
</html>




