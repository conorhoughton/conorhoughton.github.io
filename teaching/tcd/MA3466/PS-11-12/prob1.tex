\documentclass[12pt]{article}
\usepackage{a4wide, amsfonts, epsfig}

\begin{document}
\begin{center}
{\bf MA3466 Tutorial Sheet 1\footnote{Conor Houghton, {\tt houghton@maths.tcd.ie}, see also {\tt http://www.maths.tcd.ie/\char126 houghton/MA3466}}}\\[1cm]{} 27 January 2012
\end{center}
\begin{enumerate}

\item For two random variables with numerical outcomes, find $p(x,y)$ so that there is zero {\sl correllation}
\begin{equation}
\langle x'y'\rangle=0 
\end{equation}
but $X$ and $Y$ aren't independent. $x'=x-EX$ and
$y'=y-EY$. Correllation is often used as a measure of independence,
that is, as an indication that $p_{X,Y}(x,y)\not=p_X(x)p_Y(y)$ for all
$x\in{\cal X}$ and $y\in{\cal Y}$. In fact, we will see that the {\sl
  mutual information} is a true test of independence.

\item Work out the marginal distributions and the $x=a$ conditional distribution for
\begin{center}
\begin{tabular}{c|cc}
&$a$&$b$\\
\hline
1&$\frac{1}{3}$&$\frac{1}{6}$\\
2&0&$\frac{1}{4}$\\
3&$\frac{1}{8}$&$\frac{1}{8}$
\end{tabular}
\end{center}

\item (C\&T 2.1) A fair coin is flipped until the first head occurs. Let $X$ denote the number of flips required.
\begin{enumerate}
\item Find the entropy $H(X)$ in bits. The following expressions may be useful:\begin{eqnarray}
\sum_{n=0}^{\infty} r^n&=&\frac{1}{1-r}\cr
\sum_{n=0}^{\infty}nr^n&=&\frac{r}{(1-r)^2}
\end{eqnarray}
\item A random variable $X$ is drawn according to this distribution. Find an efficient sequence of yes-no questions of the form, \lq Is $X$ contained in the set $S$?\rq{}. Compare $H(X)$ to the expected number of questions required to determine $X$.
\end{enumerate}

\item (C\&T 2.3) What is the minimum value of $H(p_1,p_2,\ldots,p_n)=H({\bf p})$ as ${\bf p}$ ranges overall possible vectors. Find the ${\bf p}$ which achieve this bound. $H(p_1,p_2,\ldots,p_n)$ is a common notation for $H(X)$ where $X$ has $n$-outcomes $\{x_1,x_2,\ldots,x_n\}$ and $p_1=p(x_1)$, $p_2=p(x_2)$ and so on.


\end{enumerate}


\end{document}
