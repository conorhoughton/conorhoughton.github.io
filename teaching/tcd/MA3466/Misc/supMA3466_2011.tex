% LaTeX blank document for exam papers
% Anything after a % on a line is a comment -- does not appear in
% final document
\documentclass[12pt]{article}
\usepackage{tcdexam}
\begin{document}
\slcode{XMA}  % insert XID code for the exam, e.g., XMA3211
\course{Course MA3466}  % insert course e.g. Course 321
\examiners{Conor Houghton} % insert e.g. Dr. R. Timoney
\groups{JS/SS Maths\\JS/SS TP\\MSC in HPC}  % insert e.g. JF Engineers \\ JF MSISS
\term{2011}  % insert e.g. Trinity Term 1987
\day{       }   % insert e.g. Tuesday, June 2
\time{}  % insert e.g. 9.30 --- 12.30  or 2.00 --- 5.00
\place{}  % insert e.g. Exam Hall
\instructions{
 Credit will be given for the best three answers
   \\[12pt]
Log tables are available from the invigilators, if required.\\[12pt]
Non-programmable calculators are permitted for this
examination.}

\maketitle

% begin test of exam use \question{1} to produce a bold face number 1.
% in the left margin.  Use \part{(a)} or \part{(i)} to number parts
% of questions. Put a blank line before these if you want it to come
% on a new line.

\begin{enumerate}
\item % 1st Question
\begin{enumerate}
\item [2 marks] Define the Shannon Entropy $H(X)$ for a discrete random variable $X$.
\item [6 marks] Prove
$$H(X,Y)=H(X)+H(Y|X)$$
and
$$I(X;Y)=H(X)-H(X|Y).$$
\item [6 marks] Prove
$$D(p||q)>0$$ 
with equality if and only if $p(x)=q(x)$ for all $x\in{\cal X}$.
\item [6 marks] If ${\cal X}={a,b,c}$ and two distributions are given by $p$ and $q$ with
\begin{tabular}{l|ll}
p&$a$&$b$&c\\
\hline
&1/3&1/3&1/3
\end{tabular}
and
\begin{tabular}{l|ll}
q&$a$&$b$&c\\
\hline
&1/2&1/4&1/4
\end{tabular}
calculate the relative entropy $D(p\|q)$ and $D(q\|p)$. You can write the answer in terms of $\log{3}$ and so on.
\item [4 marks]
Are $D(p\|q)$ and $D(q\|p)$ equal in general? Are the only equal when $p(x)=q(x)$ for all $x\{\cal X}$, the set of outcomes?


\item % 2rd Question
For discrete random variables $X$, $Y$ and $Z$
\begin{enumerate} 
\item (5 marks) prove
$$H(X,Y)=H(X)+H(Y|X)$$
\item (5 marks) prove
$$I(X;Y)=H(Y)-H(Y|X)$$
\item (5 marks) prove
$$H(X,Y|Z)\ge H(X|Z)$$
\item (5 marks) prove
$$I(X;Z|Y)=I(Z;Y|X)-I(Z;Y)+I(X;Z)$$ 
\end{enumerate}

\item % 3nd Question
\begin{enumerate}
\item [4 marks] Show that for a Markov chain $X\rightarrow
  Y\rightarrow Z$ implies  $Z\rightarrow
  Y\rightarrow X$.
\item [8 marks] A random variable $X$ takes values $1$ and $0$ with
  probability $p$ and $1-p$ determined by a distribution $P$. Let $X_1$ to $X_k$ be a set of independent variables each of which has the same distribution as $X$. If 
$$N=\sum_{i=1}^k X_i$$
it is clear $P\rightarrow (X_1,\ldots,X_k)\rightarrow N$, argue that
$$P\rightarrow N\rightarrow (X_1,\ldots,X_k)$$ 
and say what is meant by a {\sl sufficient statistic}.
\item [8 marks] Show $H(X_0|X_n)$ is non-decreasing with $n$ for a Markov chain
$$X_0\rightarrow X_1\rightarrow \ldots \rightarrow X_n.$$

\item % 4th Question
\begin{enumerate}
\item [5 marks] Define a source code and an instantaneous code. Define the expected length $L(C)$ of a source code $C(x)$.
\item [10 marks] State and prove the Kraft inequality.
\item [5 marks] Work out Huffman codes for
$$
\item $p(A)=.4$, $p(B)=.2$, $p(C)=.2$, $p(D)=.1$ and $p(E)=.1$ with $D=2$.
$$
Work out the average code length.
\end{enumerate}



% end of document should have this next line
\end{document}
