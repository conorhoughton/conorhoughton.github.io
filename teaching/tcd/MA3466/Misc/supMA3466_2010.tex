% LaTeX blank document for exam papers
% Anything after a % on a line is a comment -- does not appear in
% final document
\documentclass[12pt]{article}
\usepackage{tcdexam}
\begin{document}
\slcode{XMA}  % insert XID code for the exam, e.g., XMA3211
\course{Course MA3466}  % insert course e.g. Course 321
\examiners{Conor Houghton} % insert e.g. Dr. R. Timoney
\groups{JS/SS Maths\\JS/SS TP\\MSC in HPC}  % insert e.g. JF Engineers \\ JF MSISS
\term{2008}  % insert e.g. Trinity Term 1987
\day{       }   % insert e.g. Tuesday, June 2
\time{}  % insert e.g. 9.30 --- 12.30  or 2.00 --- 5.00
\place{}  % insert e.g. Exam Hall
\instructions{
 Credit will be given for the best three answers
   \\[12pt]
Log tables are available from the invigilators, if required.\\[12pt]
Non-programmable calculators are permitted for this
examination.}

\maketitle

% begin test of exam use \question{1} to produce a bold face number 1.
% in the left margin.  Use \part{(a)} or \part{(i)} to number parts
% of questions. Put a blank line before these if you want it to come
% on a new line.

\begin{enumerate}
\item % 1st Question
\begin{enumerate}
\item [6 marks] Define
\begin{enumerate}
\item The Shannon Entropy.
\item The conditional entropy.
\item The relative entropy, also known as the Kullback-Leibler divergence.
\item The mutual information.
\end{enumerate}
\item [4 marks] Prove
$$H(X,Y)=H(X)+H(Y|X)$$
and
$$I(X;Y)=H(X)-H(X|Y).$$
\item [6 marks] If ${\cal X}={a,b,c}$ and two distributions are given by $p$ and $q$ with
\begin{tabular}{l|ll}
p&$a$&$b$&c\\
\hline
&1/3&1/3&1/3
\end{tabular}
and
\begin{tabular}{l|ll}
q&$a$&$b$&c\\
\hline
&1/2&1/4&1/4
\end{tabular}
calculate the relative entropy $D(p\|q)$ and $D(q\|p)$. You can write the answer in terms of $\log{3}$ and so on.
\item [4 marks]
Are $D(p\|q)$ and $D(q\|p)$ equal in general? Are the only equal when $p(x)=q(x)$ for all $x\{\cal X}$, the set of outcomes?



\item % 2rd Question
For discrete random variables $X$, $Y$ and $Z$
\begin{enumerate} 
\item [5 marks] prove
$$H(X,Y)=H(X)+H(Y|X)$$
\item [5 marks] prove
$$I(X,Y;Z)=I(Y;Z|X)+I(X;Z)$$
\item [10 marks] Give examples of joint random variables $X$, $Y$ and $Z$ such that
\begin{enumerate}
\item $I(X;Y|Z)< I(X;Y)$
\item $I(X;Y|Z)> I(X;Y)$
\end{enumerate}
\end{enumerate}

\begin{enumerate}
\item [4 marks] What is meant by a Markov chain $X\rightarrow
  Y\rightarrow Z$ and 
\item [3 marks] Show that $X\rightarrow Y\rightarrow Z$ implies
  $Z\rightarrow Y\rightarrow X$.
\item [8 marks] State and prove the data processing inequality.
\item [5 marks] Suppose that a Markov chain starts in one of $n$ states, necks down to $k<n$ states and then fans back out to $m>k$ states. Show that the dependence of the first and last variables, $X$ and $Z$ is limited by the bottleneck by showing $I(X,Z)\le\log{k}$.
\end{enumerate}

\item % 4th Question
\begin{enumerate}
\item [5 marks] Define a source code and an instantaneous code. Define the expected length $L(C)$ of a source code $C(x)$.
\item [10 marks] State and prove the Kraft inequality.
\item [5 marks] Work out Huffman codes for
$$
\item $p(A)=.4$, $p(B)=.2$, $p(C)=.2$, $p(D)=.1$ and $p(E)=.1$ with $D=2$.
$$
Work out the average code length.
\end{enumerate}



% end of document should have this next line
\end{document}
