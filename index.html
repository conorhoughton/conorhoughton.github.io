

<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
<html>
<font face="verdana, Sans-serif">
<head>
   <title>Conor Houghton</title>
   <link rel="icon"  type="image/x-icon" href="https://conorhoughton.github.io/small_star.ico?">
</head>


<body  bgcolor=white link="slategrey" vlink="darkslategrey" alink="darkslategrey">

<table>
  <tbody>
    <tr>
      <td bgcolor="slategrey" width="30">&nbsp;</td>
      
      <td bgcolor="darkslategrey" width="1">&nbsp;</td>
      <td bgcolor=white width="10">&nbsp;</td>

     <td width="15"> 
<td>
  <h1 style="color: darkslategrey">Conor Houghton</h1>
  <table>
    <tbody>
      <tr>
	<td><img src="headshot.jpg" type="image/jpeg" style="max-width:200px;" alt="Conor Houghton" /></td>
	<td width="30"></td>
	<td>
<p><b>Contact</b></p>
<blockquote>
<a href="mailto:conor.houghton@bristol.ac.uk">conor.houghton@bristol.ac.uk</a><p>    
School of Engineering Mathematics and Technology<br> 
University of Bristol<br> 
Michael Ventris Building<br>
Woodland Road<br>
Bristol<br>
BS8 1UB<br>
England<p>
<a href="https://research-information.bris.ac.uk/en/persons/conor-houghton">official home page</a>.
</p>
</blockquote>
	</td>
    </tbody>
  </table>
  <p><br></p>
<a href="papers"><b>Papers</b></a><br>
  <p><br></p>


  <p><b>The latest news!</b></p>
  <p><b>(2024-11-12)</b>Paper news:</p>
<p>We have just put a revised version of</p>
<p><i>An iterated learning model of language change that mixes
supervised and unsupervised learning</i><br>
Jack Bunyan, Seth Bullock, Conor Houghton</p>
<p>up on arXiv:</p>
  <p><a href="https://arxiv.org/abs/2405.20818">arxiv: 2405.20818</a></p>
<p>The good news is that the paper is now in press with <b>PLoS Complex Systems</b>, which I am super excited about. This is the paper I think of as "the big ILM paper" and it is great we got it into a good journal, I really keen for people to read it and the venue helps with that!</p>

  <p>The ILM, or iterated learning model, was used by Simon Kirby and his co-workers to model the way languages evolve; the "languages" in the model are comically simple, maps from vectors of zeros and ones representing "meanings" to vectors of zeros and ones representing "signals". Simple as they are, we can nonetheless ask how expressive and composition one of these languages is. </p>

  <p>In the ILM a tutor teaches a pupil a language using a limited random set of examples; the pupil then become a tutor itself and teaches a new pupil. Since it only learned a subset of the language it is forced to generalize to produce its own examples. The interesting thing is that the language evolves expressivity and compositionality through the resulting cycles of generalization and subselection</p> 

<p>The original ILM included an unrealistic step, basically to make a map that goes both ways, mapping meanings to signals and signals to meaning, the pupil needed to do a sort of inversion. It is as if a child stayed silent until 12, then imagined everything they could ever possibly say and then did some complicated matrix operation in their heads before starting to speak. We found a way of removing this step, but to do that, we need the pupil to cogitate on meanings, to sharpen their understanding of the language they are learning by mapping meanings into signals, then mapping the signals back to meanings to check the output meaning was the same as the input meaning.</p>

<p><b>My new belief</b> is that this is the crucial step for the appearance of language. The hypothesis is that at some point in our evolutionary past we internalized our utterance and from that unfolded language. </p>
<p>Anyway I have a short talk about it here:</p>
<p><a href="https://www.youtube.com/watch?v=kRRqZ-l9K9A">youtube</a></p>
<p><br></p>
  <p><b><a href="./old.html">Old news</a><b></b>
<p><br></p>
</td>
  </tbody>
</table>
</body>
</html>




